{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "pixel_cnn_cifar.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ac6b470249e04168b91c87229971a6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d368c8fd1c974710b65dc66c91e7b80e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e2bb39d67a914bb7aa54ff2673685787",
              "IPY_MODEL_aaf44724c6f44ba9bd62f671fb47de45"
            ]
          }
        },
        "d368c8fd1c974710b65dc66c91e7b80e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e2bb39d67a914bb7aa54ff2673685787": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e7dc38af23434ad29178ea67c503caea",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1b8374489f87455299488be608f5c131"
          }
        },
        "aaf44724c6f44ba9bd62f671fb47de45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_89984ab5ee4e46758ae88768ab6a2f52",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 33021921.71it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dfed3456f5884533b38f7051f366ddce"
          }
        },
        "e7dc38af23434ad29178ea67c503caea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1b8374489f87455299488be608f5c131": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "89984ab5ee4e46758ae88768ab6a2f52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dfed3456f5884533b38f7051f366ddce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pxMlo2FCaJiP",
        "outputId": "2764289a-674a-413a-d84c-d86a22976950"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "import warnings\n",
        "# import UserWarning\n",
        "\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.cuda.get_device_name(0)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla V100-SXM2-16GB'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "ac6b470249e04168b91c87229971a6a9",
            "d368c8fd1c974710b65dc66c91e7b80e",
            "e2bb39d67a914bb7aa54ff2673685787",
            "aaf44724c6f44ba9bd62f671fb47de45",
            "e7dc38af23434ad29178ea67c503caea",
            "1b8374489f87455299488be608f5c131",
            "89984ab5ee4e46758ae88768ab6a2f52",
            "dfed3456f5884533b38f7051f366ddce"
          ]
        },
        "id": "UCJePAU9aWAH",
        "outputId": "2e72d39c-67fb-4f3d-dde9-f09943d44548"
      },
      "source": [
        "\n",
        "training_set = torch.utils.data.DataLoader(torchvision.datasets.CIFAR10('data', train=True, download=True, transform=torchvision.transforms.ToTensor()),\n",
        "                     batch_size=32, shuffle=True)\n",
        "validation_set = torch.utils.data.DataLoader(torchvision.datasets.CIFAR10('data', train=False, download=True, transform=torchvision.transforms.ToTensor()),\n",
        "                     batch_size=32, shuffle=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac6b470249e04168b91c87229971a6a9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnnmcnLEn1kB"
      },
      "source": [
        "\n",
        "\n",
        "class MaskedConv2d(torch.nn.Conv2d):\n",
        "    def __init__(self, mask_type, *arg, **kwargs):\n",
        "        super().__init__(*arg, **kwargs)\n",
        "        assert mask_type in [\"A\", \"B\"], \"Invalid mask type\"\n",
        "        self.batch_size, self.channels, self.height, self.width = self.weight.data.shape\n",
        "        self.mask_type = mask_type\n",
        "        \n",
        "        in_red_channel_index = torch.arange(0, self.in_channels, 3)\n",
        "        in_green_channel_index = torch.arange(1, self.in_channels, 3)\n",
        "        in_blue_channel_index = torch.arange(2, self.in_channels, 3)\n",
        "        \n",
        "        out_red_channel_index = torch.arange(0, self.out_channels, 3)\n",
        "        out_green_channel_index = torch.arange(1, self.out_channels, 3)\n",
        "        out_blue_channel_index = torch.arange(2, self.out_channels, 3)\n",
        "\n",
        "        # in_red_channel_index = torch.arange(0, self.in_channels//3*1)\n",
        "        # in_green_channel_index = torch.arange(self.in_channels//3*1, self.in_channels//3*2)\n",
        "        # in_blue_channel_index = torch.arange(self.in_channels//3*2, self.in_channels)\n",
        "        \n",
        "        # out_red_channel_index = torch.arange(0, self.out_channels//3*1)\n",
        "        # out_green_channel_index = torch.arange(self.out_channels//3*1, self.out_channels//3*2)\n",
        "        # out_blue_channel_index = torch.arange(self.out_channels//3*2, self.out_channels)\n",
        "        \n",
        "        self.mask = torch.ones_like(self.weight.data)\n",
        "        self.mask[:,:,self.height//2:, self.width//2:] = 0\n",
        "        self.mask[:, :, self.height//2+1:, :] = 0\n",
        "        red_mask_template = self.mask[0].clone()\n",
        "        green_mask_template = self.mask[0].clone()\n",
        "        blue_mask_template = self.mask[0].clone()\n",
        "        if mask_type == \"A\":\n",
        "#             self.mask[red_channel_index, :, self.height//2+1:, :] = 0\n",
        "            green_mask_template[in_red_channel_index, self.height//2, self.width//2] = 1\n",
        "            self.mask[out_green_channel_index] = green_mask_template\n",
        "            blue_mask_template[torch.cat((in_red_channel_index, in_green_channel_index)), self.height//2, self.width//2] = 1\n",
        "            self.mask[out_blue_channel_index] = blue_mask_template.clone()\n",
        "        \n",
        "        elif mask_type == \"B\":\n",
        "            red_mask_template[in_red_channel_index, self.height//2, self.width//2] = 1\n",
        "            self.mask[out_red_channel_index] = red_mask_template\n",
        "            green_mask_template[torch.cat((in_red_channel_index, in_green_channel_index)), self.height//2, self.width//2] = 1\n",
        "            self.mask[out_green_channel_index] = green_mask_template\n",
        "            blue_mask_template[:, self.height//2, self.width//2] = 1\n",
        "            self.mask[out_blue_channel_index] = blue_mask_template\n",
        "        self.mask = self.mask.to(device=device)\n",
        "        \n",
        "            \n",
        "    def forward(self, images):\n",
        "        self.weight.data = self.weight.data * self.mask\n",
        "        return super(MaskedConv2d, self).forward(images)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE94UfmFoHhE"
      },
      "source": [
        "\n",
        "\n",
        "class PixelCNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, hidden_channel, out_channels):\n",
        "        super(PixelCNN, self).__init__()\n",
        "        self.hidden_channel = hidden_channel\n",
        "        self.conv1 = MaskedConv2d(mask_type='A', in_channels=in_channels,  out_channels=hidden_channel, kernel_size=7, stride=1, padding=3)\n",
        "        self.batch_norm1 = torch.nn.BatchNorm2d(hidden_channel)\n",
        "\n",
        "        self.conv2 = MaskedConv2d(mask_type='B', in_channels=hidden_channel,  out_channels=hidden_channel, kernel_size=7, stride=1, padding=3)\n",
        "        self.batch_norm2 = torch.nn.BatchNorm2d(hidden_channel)\n",
        "        \n",
        "        self.conv3 = MaskedConv2d(mask_type='B', in_channels=hidden_channel,  out_channels=hidden_channel, kernel_size=7, stride=1, padding=3)\n",
        "        self.batch_norm3 = torch.nn.BatchNorm2d(hidden_channel)\n",
        "\n",
        "        self.conv4 = MaskedConv2d(mask_type='B', in_channels=hidden_channel,  out_channels=hidden_channel, kernel_size=7, stride=1, padding=3)\n",
        "        self.batch_norm4 = torch.nn.BatchNorm2d(hidden_channel)\n",
        "\n",
        "        self.conv5 = MaskedConv2d(mask_type='B', in_channels=hidden_channel,  out_channels=hidden_channel, kernel_size=7, stride=1, padding=3)\n",
        "        self.batch_norm5 = torch.nn.BatchNorm2d(hidden_channel)\n",
        "\n",
        "        self.conv6 = MaskedConv2d(mask_type='B', in_channels=hidden_channel,  out_channels=hidden_channel, kernel_size=7, stride=1, padding=3)\n",
        "        self.batch_norm6 = torch.nn.BatchNorm2d(hidden_channel)\n",
        "\n",
        "        self.conv7 = MaskedConv2d(mask_type='B', in_channels=hidden_channel,  out_channels=hidden_channel, kernel_size=7, stride=1, padding=3)\n",
        "        self.batch_norm7 = torch.nn.BatchNorm2d(hidden_channel)\n",
        "\n",
        "        self.conv8 = MaskedConv2d(mask_type='B', in_channels=hidden_channel,  out_channels=hidden_channel, kernel_size=7, stride=1, padding=3)\n",
        "        self.batch_norm8 = torch.nn.BatchNorm2d(hidden_channel)\n",
        "\n",
        "        self.conv9 = MaskedConv2d(mask_type='B', in_channels=hidden_channel,  out_channels=hidden_channel, kernel_size=7, stride=1, padding=3)\n",
        "        self.batch_norm9 = torch.nn.BatchNorm2d(hidden_channel)\n",
        "\n",
        "        self.conv10 = MaskedConv2d(mask_type='B', in_channels=hidden_channel,  out_channels=hidden_channel, kernel_size=7, stride=1, padding=3)\n",
        "        self.batch_norm10 = torch.nn.BatchNorm2d(hidden_channel)\n",
        "\n",
        "        self.conv11 = MaskedConv2d(mask_type='B', in_channels=hidden_channel,  out_channels=hidden_channel, kernel_size=7, stride=1, padding=3)\n",
        "        self.batch_norm11 = torch.nn.BatchNorm2d(hidden_channel)\n",
        "\n",
        "        self.conv12 = MaskedConv2d(mask_type='B', in_channels=hidden_channel,  out_channels=hidden_channel, kernel_size=7, stride=1, padding=3)\n",
        "        self.batch_norm12 = torch.nn.BatchNorm2d(hidden_channel)\n",
        "\n",
        "        self.conv13 = MaskedConv2d(mask_type='B', in_channels=hidden_channel,  out_channels=hidden_channel, kernel_size=7, stride=1, padding=3)\n",
        "        self.batch_norm13 = torch.nn.BatchNorm2d(hidden_channel)\n",
        "\n",
        "        self.conv14 = MaskedConv2d(mask_type='B', in_channels=hidden_channel,  out_channels=1024*3, kernel_size=1, stride=1, padding=0)\n",
        "        self.batch_norm14 = torch.nn.BatchNorm2d(1024*3)\n",
        "\n",
        "        self.conv15 = MaskedConv2d(mask_type='B', in_channels=1024*3,  out_channels=1024*3, kernel_size=1, stride=1, padding=0)\n",
        "        self.batch_norm15 = torch.nn.BatchNorm2d(1024*3)\n",
        "\n",
        "        self.conv_red = torch.nn.Conv2d(in_channels=1024, out_channels=out_channels, kernel_size=1)\n",
        "        self.conv_green = torch.nn.Conv2d(in_channels=1024, out_channels=out_channels, kernel_size=1)\n",
        "        self.conv_blue = torch.nn.Conv2d(in_channels=1024, out_channels=out_channels, kernel_size=1)\n",
        "\n",
        "        # self.conv16 = torch.nn.Conv2d(in_channels=1026,  out_channels=256*3, kernel_size=1, stride=1, padding=0)\n",
        "        self.red_channel_index = torch.arange(0, 1024*3, 3).long()\n",
        "        self.green_channel_index = torch.arange(1, 1024*3, 3).long()\n",
        "        self.blue_channel_index = torch.arange(2, 1024*3, 3).long()\n",
        "\n",
        "\n",
        "            \n",
        "    def forward(self, images):\n",
        "        pred = self.conv1(images)\n",
        "        pred = self.batch_norm1(pred)\n",
        "        pred = torch.nn.ReLU()(pred)\n",
        "\n",
        "        pred = self.conv2(pred)\n",
        "        pred = self.batch_norm2(pred)\n",
        "        pred = torch.nn.ReLU()(pred)\n",
        "\n",
        "        pred = self.conv3(pred)\n",
        "        pred = self.batch_norm3(pred)\n",
        "        pred = torch.nn.ReLU()(pred)\n",
        "\n",
        "        pred = self.conv4(pred)\n",
        "        pred = self.batch_norm4(pred)\n",
        "        pred = torch.nn.ReLU()(pred)\n",
        "\n",
        "        pred = self.conv5(pred)\n",
        "        pred = self.batch_norm5(pred)\n",
        "        pred = torch.nn.ReLU()(pred)\n",
        "\n",
        "        pred = self.conv6(pred)\n",
        "        pred = self.batch_norm6(pred)\n",
        "        pred = torch.nn.ReLU()(pred)\n",
        "\n",
        "        pred = self.conv7(pred)\n",
        "        pred = self.batch_norm7(pred)\n",
        "        pred = torch.nn.ReLU()(pred)\n",
        "\n",
        "        pred = self.conv8(pred)\n",
        "        pred = self.batch_norm8(pred)\n",
        "        pred = torch.nn.ReLU()(pred)\n",
        "\n",
        "        pred = self.conv9(pred)\n",
        "        pred = self.batch_norm9(pred)\n",
        "        pred = torch.nn.ReLU()(pred)\n",
        "\n",
        "        pred = self.conv10(pred)\n",
        "        pred = self.batch_norm10(pred)\n",
        "        pred = torch.nn.ReLU()(pred)\n",
        "\n",
        "        pred = self.conv11(pred)\n",
        "        pred = self.batch_norm11(pred)\n",
        "        pred = torch.nn.ReLU()(pred)\n",
        "\n",
        "        pred = self.conv12(pred)\n",
        "        pred = self.batch_norm12(pred)\n",
        "        pred = torch.nn.ReLU()(pred)\n",
        "\n",
        "        pred = self.conv13(pred)\n",
        "        pred = self.batch_norm13(pred)\n",
        "        pred = torch.nn.ReLU()(pred)\n",
        "\n",
        "        pred = self.conv14(pred)\n",
        "        pred = self.batch_norm14(pred)\n",
        "        pred = torch.nn.ReLU()(pred)\n",
        "\n",
        "        pred = self.conv15(pred)\n",
        "        pred = self.batch_norm15(pred)\n",
        "        pred = torch.nn.ReLU()(pred)\n",
        "\n",
        "        # print(pred.shape)\n",
        "        # print(self.red_channel_index )\n",
        "        red_channel = pred[:, self.red_channel_index]\n",
        "        green_channel = pred[:, self.green_channel_index]\n",
        "        blue_channel = pred[:, self.blue_channel_index]\n",
        "\n",
        "        # print(red_channel.shape)\n",
        "        # print(green_channel.shape)\n",
        "        # print(blue_channel.shape)\n",
        "\n",
        "        red_pred = self.conv_red(red_channel)\n",
        "        green_pred = self.conv_green(green_channel)\n",
        "        blue_pred = self.conv_blue(blue_channel)\n",
        "\n",
        "        # print(red_pred.shape)\n",
        "        # print(green_pred.shape)\n",
        "        # print(blue_pred.shape)\n",
        "        pred = torch.stack((red_pred, green_pred, blue_pred), dim=2)\n",
        "\n",
        "        # pred = self.conv16(pred)\n",
        "        # pred = pred.reshape(images.shape[0], 256, 3, 32, 32)\n",
        "        return pred\n",
        "\n",
        "    def sample(self, shape, count, label=None, device='cuda'):\n",
        "        channels, height, width = shape\n",
        "\n",
        "        samples = torch.zeros(count, *shape).to(device)\n",
        "        if label is None:\n",
        "            labels = torch.randint(high=10, size=(count,)).to(device)\n",
        "        else:\n",
        "            labels = (label*torch.ones(count)).to(device).long()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in tqdm(range(height)):\n",
        "                for j in range(width):\n",
        "                    for c in range(channels):\n",
        "                        unnormalized_probs = self.forward(samples)\n",
        "                        pixel_probs = torch.softmax(unnormalized_probs[:, :, c, i, j], dim=1)\n",
        "                        sampled_levels = torch.multinomial(pixel_probs, 1).squeeze().float() / 255\n",
        "                        samples[:, c, i, j] = sampled_levels\n",
        "\n",
        "        return samples\n",
        "\n",
        "\n",
        "pixel_cnn = PixelCNN(in_channels=3, hidden_channel=128*3, out_channels=256)\n",
        "pixel_cnn.to(device=device)\n",
        "optim = torch.optim.Adam(params=pixel_cnn.parameters())\n",
        "train_loss = []\n",
        "validation_loss = []\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSQ8fASP4pXK"
      },
      "source": [
        "\n",
        "def save_model(epoch, model, optim, train_loss, save_path):\n",
        "    check_point = {\n",
        "        \"epoch\": epoch, \n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optim_state_dict\": optim.state_dict(),\n",
        "        \"train_loss\": train_loss,\n",
        "    }\n",
        "    torch.save(check_point, save_path)\n",
        "    print(\"Save model succesfully\")\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWZknmeHaetO"
      },
      "source": [
        "\n",
        "\n",
        "def generate_images(nb_images, images_shape, model, device=\"cpu\"):\n",
        "    generated_images = torch.zeros((nb_images, images_shape[2], images_shape[0], images_shape[1]), device=\"cuda\").float()\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(images_shape[0])):\n",
        "            for j in range(images_shape[1]):\n",
        "                for c in range(images_shape[2]):\n",
        "                    pred = pixel_cnn(generated_images)\n",
        "                    # print(pred.shape)\n",
        "                    # pred = pred.reshape(16, 256, 3, 32, 32)\n",
        "                    # print(pred.shape)\n",
        "                    prob = torch.nn.functional.softmax(pred[:,:,c,i,j], dim=1)\n",
        "                    generated_images[:,c,i,j] = torch.multinomial(prob, 1).float().flatten() \n",
        "                # blue_pred = pixel_cnn(generated_images, channel=\"B\")\n",
        "                # blue_prob = torch.nn.functional.softmax(blue_pred[:,:,i,j], dim=1)\n",
        "                # generated_images[:,2,i,j] = torch.multinomial(blue_prob, 1).float().flatten() \n",
        "\n",
        "    generated_images = torchvision.utils.make_grid(generated_images, nrow=int(np.sqrt(nb_images)))\n",
        "    generated_images = generated_images.permute(1,2,0).detach().cpu().numpy() / 255.0\n",
        "    plt.figure(figsize=(7.5,7.5))\n",
        "    plt.imshow(generated_images)\n",
        "    plt.title(\"Generated image\")\n",
        "    plt.show()\n",
        "    return generated_images\n",
        "\n",
        "# generated_images = generate_images(16, (32,32,3), pixel_cnn, device=device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvHx2rFKabgN"
      },
      "source": [
        "\n",
        "# a = generated_images[10].permute(1,2,0).detach().cpu().numpy()\n",
        "# plt.imshow(a/255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eda92KLunkZO"
      },
      "source": [
        "\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch}\\n\")\n",
        "    # Training \n",
        "    print(\"Start traning....\")\n",
        "    epoch_train_loss = []\n",
        "    pixel_cnn.train()\n",
        "    for images, labels in tqdm(training_set):\n",
        "        images = images.to(device=device) \n",
        "        pred = pixel_cnn(images)\n",
        "        # print(pred.shape)\n",
        "#         pred = pred.reshape(images.shape[0], 256, 3, 32, 32)\n",
        "        loss = torch.nn.CrossEntropyLoss()(pred, (images*255.0).long())\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        epoch_train_loss.append(loss.item())\n",
        "    \n",
        "    print(f\"Finish epoch {epoch}. Train_loss = {np.mean(epoch_train_loss)}\")\n",
        "    # if (epoch % 5 == 0):\n",
        "    #     save_model(epoch, pixel_cnn, optim, train_loss, f\"./drive/MyDrive/pred_cnn/pixel_cnn{epoch}.pt\")\n",
        "\n",
        "    generate_images(nb_images=36, images_shape=(32,32,3), model=pixel_cnn, device=device)\n",
        "    print(\"------------------------------------------------------------------------------------\")\n",
        "    print(\"\\n\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvSjSOcRQdRE"
      },
      "source": [
        "\n",
        "# check_point = torch.load(\"./drive/MyDrive/pred_cnn/pixel_cnn.pt\")\n",
        "\n",
        "# print(list(check_point.keys()))\n",
        "# pixel_cnn.load_state_dict(check_point[\"model_state_dict\"])\n",
        "# optim.load_state_dict(check_point[\"optim_state_dict\"])\n",
        "# train_loss = check_point[\"train_loss\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doNIIm-9V8_L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}