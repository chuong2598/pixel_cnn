{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CroppedConv2d(nn.Conv2d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CroppedConv2d, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = super(CroppedConv2d, self).forward(x)\n",
    "\n",
    "        kernel_height, _ = self.kernel_size\n",
    "        res = x[:, :, 1:-kernel_height, :]\n",
    "        shifted_up_res = x[:, :, :-kernel_height-1, :]\n",
    "\n",
    "        return res, shifted_up_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2, 6])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, *args, mask_type, data_channels, **kwargs):\n",
    "        super(MaskedConv2d, self).__init__(*args, **kwargs)\n",
    "\n",
    "        assert mask_type in ['A', 'B'], 'Invalid mask type.'\n",
    "\n",
    "        out_channels, in_channels, height, width = self.weight.size()\n",
    "        yc, xc = height // 2, width // 2\n",
    "\n",
    "        mask = np.zeros(self.weight.size(), dtype=np.float32)\n",
    "        mask[:, :, :yc, :] = 1\n",
    "        mask[:, :, yc, :xc + 1] = 1\n",
    "\n",
    "        def cmask(out_c, in_c):\n",
    "            a = (np.arange(out_channels) % data_channels == out_c)[:, None]\n",
    "            b = (np.arange(in_channels) % data_channels == in_c)[None, :]\n",
    "            return a * b\n",
    "\n",
    "        for o in range(data_channels):\n",
    "            for i in range(o + 1, data_channels):\n",
    "                mask[cmask(o, i), yc, xc] = 0\n",
    "\n",
    "        if mask_type == 'A':\n",
    "            for c in range(data_channels):\n",
    "                mask[cmask(c, c), yc, xc] = 0\n",
    "\n",
    "        mask = torch.from_numpy(mask).float()\n",
    "\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        x = super(MaskedConv2d, self).forward(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from .conv_layers import MaskedConv2d, CroppedConv2d\n",
    "\n",
    "\n",
    "class CausalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, data_channels):\n",
    "        super(CausalBlock, self).__init__()\n",
    "        self.split_size = out_channels\n",
    "\n",
    "        self.v_conv = CroppedConv2d(in_channels,\n",
    "                                    2 * out_channels,\n",
    "                                    (kernel_size // 2 + 1, kernel_size),\n",
    "                                    padding=(kernel_size // 2 + 1, kernel_size // 2))\n",
    "        self.v_fc = nn.Conv2d(in_channels,\n",
    "                              2 * out_channels,\n",
    "                              (1, 1))\n",
    "        self.v_to_h = nn.Conv2d(2 * out_channels,\n",
    "                                2 * out_channels,\n",
    "                                (1, 1))\n",
    "\n",
    "        self.h_conv = MaskedConv2d(in_channels,\n",
    "                                   2 * out_channels,\n",
    "                                   (1, kernel_size),\n",
    "                                   mask_type='A',\n",
    "                                   data_channels=data_channels,\n",
    "                                   padding=(0, kernel_size // 2))\n",
    "        self.h_fc = MaskedConv2d(out_channels,\n",
    "                                 out_channels,\n",
    "                                 (1, 1),\n",
    "                                 mask_type='A',\n",
    "                                 data_channels=data_channels)\n",
    "\n",
    "    def forward(self, image):\n",
    "        v_out, v_shifted = self.v_conv(image)\n",
    "        v_out += self.v_fc(image)\n",
    "        v_out_tanh, v_out_sigmoid = torch.split(v_out, self.split_size, dim=1)\n",
    "        v_out = torch.tanh(v_out_tanh) * torch.sigmoid(v_out_sigmoid)\n",
    "\n",
    "        h_out = self.h_conv(image)\n",
    "        v_shifted = self.v_to_h(v_shifted)\n",
    "        h_out += v_shifted\n",
    "        h_out_tanh, h_out_sigmoid = torch.split(h_out, self.split_size, dim=1)\n",
    "        h_out = torch.tanh(h_out_tanh) * torch.sigmoid(h_out_sigmoid)\n",
    "        h_out = self.h_fc(h_out)\n",
    "\n",
    "        return v_out, h_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GatedBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, data_channels):\n",
    "        super(GatedBlock, self).__init__()\n",
    "        self.split_size = out_channels\n",
    "\n",
    "        self.v_conv = CroppedConv2d(in_channels,\n",
    "                                    2 * out_channels,\n",
    "                                    (kernel_size // 2 + 1, kernel_size),\n",
    "                                    padding=(kernel_size // 2 + 1, kernel_size // 2))\n",
    "        self.v_fc = nn.Conv2d(in_channels,\n",
    "                              2 * out_channels,\n",
    "                              (1, 1))\n",
    "                              \n",
    "        self.v_to_h = MaskedConv2d(2 * out_channels,\n",
    "                                   2 * out_channels,\n",
    "                                   (1, 1),\n",
    "                                   mask_type='B',\n",
    "                                   data_channels=data_channels)\n",
    "\n",
    "        self.h_conv = MaskedConv2d(in_channels,\n",
    "                                   2 * out_channels,\n",
    "                                   (1, kernel_size),\n",
    "                                   mask_type='B',\n",
    "                                   data_channels=data_channels,\n",
    "                                   padding=(0, kernel_size // 2))\n",
    "        self.h_fc = MaskedConv2d(out_channels,\n",
    "                                 out_channels,\n",
    "                                 (1, 1),\n",
    "                                 mask_type='B',\n",
    "                                 data_channels=data_channels)\n",
    "\n",
    "        self.h_skip = MaskedConv2d(out_channels,\n",
    "                                   out_channels,\n",
    "                                   (1, 1),\n",
    "                                   mask_type='B',\n",
    "                                   data_channels=data_channels)\n",
    "\n",
    "        self.label_embedding = nn.Embedding(10, 2*out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        v_in, h_in, skip, label = x[0], x[1], x[2], x[3]\n",
    "\n",
    "        label_embedded = self.label_embedding(label).unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "        v_out, v_shifted = self.v_conv(v_in)\n",
    "        v_out += self.v_fc(v_in)\n",
    "        v_out += label_embedded\n",
    "        v_out_tanh, v_out_sigmoid = torch.split(v_out, self.split_size, dim=1)\n",
    "        v_out = torch.tanh(v_out_tanh) * torch.sigmoid(v_out_sigmoid)\n",
    "\n",
    "        h_out = self.h_conv(h_in)\n",
    "        v_shifted = self.v_to_h(v_shifted)\n",
    "        h_out += v_shifted\n",
    "        h_out += label_embedded\n",
    "        h_out_tanh, h_out_sigmoid = torch.split(h_out, self.split_size, dim=1)\n",
    "        h_out = torch.tanh(h_out_tanh) * torch.sigmoid(h_out_sigmoid)\n",
    "\n",
    "        # skip connection\n",
    "        skip = skip + self.h_skip(h_out)\n",
    "\n",
    "        h_out = self.h_fc(h_out)\n",
    "\n",
    "        # residual connections\n",
    "        h_out = h_out + h_in\n",
    "        v_out = v_out + v_in\n",
    "\n",
    "        return {0: v_out, 1: h_out, 2: skip, 3: label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(PixelCNN, self).__init__()\n",
    "\n",
    "        DATA_CHANNELS = 3\n",
    "\n",
    "        self.hidden_fmaps = cfg.hidden_fmaps\n",
    "        self.color_levels = cfg.color_levels\n",
    "\n",
    "        self.causal_conv = CausalBlock(DATA_CHANNELS,\n",
    "                                       cfg.hidden_fmaps,\n",
    "                                       cfg.causal_ksize,\n",
    "                                       data_channels=DATA_CHANNELS)\n",
    "\n",
    "        self.hidden_conv = nn.Sequential(\n",
    "            *[GatedBlock(cfg.hidden_fmaps, cfg.hidden_fmaps, cfg.hidden_ksize, DATA_CHANNELS) for _ in range(cfg.hidden_layers)]\n",
    "        )\n",
    "\n",
    "        self.label_embedding = nn.Embedding(10, self.hidden_fmaps)\n",
    "\n",
    "        self.out_hidden_conv = MaskedConv2d(cfg.hidden_fmaps,\n",
    "                                            cfg.out_hidden_fmaps,\n",
    "                                            (1, 1),\n",
    "                                            mask_type='B',\n",
    "                                            data_channels=DATA_CHANNELS)\n",
    "\n",
    "        self.out_conv = MaskedConv2d(cfg.out_hidden_fmaps,\n",
    "                                     DATA_CHANNELS * cfg.color_levels,\n",
    "                                     (1, 1),\n",
    "                                     mask_type='B',\n",
    "                                     data_channels=DATA_CHANNELS)\n",
    "\n",
    "    def forward(self, image, label):\n",
    "        count, data_channels, height, width = image.size()\n",
    "\n",
    "        v, h = self.causal_conv(image)\n",
    "\n",
    "        _, _, out, _ = self.hidden_conv({0: v,\n",
    "                                         1: h,\n",
    "                                         2: image.new_zeros((count, self.hidden_fmaps, height, width), requires_grad=True),\n",
    "                                         3: label}).values()\n",
    "\n",
    "        label_embedded = self.label_embedding(label).unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "        # add label bias\n",
    "        out += label_embedded\n",
    "        out = F.relu(out)\n",
    "        out = F.relu(self.out_hidden_conv(out))\n",
    "        out = self.out_conv(out)\n",
    "\n",
    "        out = out.view(count, self.color_levels, data_channels, height, width)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def sample(self, shape, count, label=None, device='cuda'):\n",
    "        channels, height, width = shape\n",
    "\n",
    "        samples = torch.zeros(count, *shape).to(device)\n",
    "        if label is None:\n",
    "            labels = torch.randint(high=10, size=(count,)).to(device)\n",
    "        else:\n",
    "            labels = (label*torch.ones(count)).to(device).long()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(height):\n",
    "                for j in range(width):\n",
    "                    for c in range(channels):\n",
    "                        unnormalized_probs = self.forward(samples, labels)\n",
    "                        pixel_probs = torch.softmax(unnormalized_probs[:, :, c, i, j], dim=1)\n",
    "                        sampled_levels = torch.multinomial(pixel_probs, 1).squeeze().float() / (self.color_levels - 1)\n",
    "                        samples[:, c, i, j] = sampled_levels\n",
    "\n",
    "        return samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_masked_conv = CroppedConv2d(in_channels=3, out_channels=5, kernel_size=3)\n",
    "test_tensor = torch.randn(1,3,8,8)\n",
    "res, shift = test_masked_conv(test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 2, 6])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 2, 6])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
